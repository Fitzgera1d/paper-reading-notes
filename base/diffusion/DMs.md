# 论文标题: Deep Unsupervised Learning using Nonequilibrium Thermodynamics - arXiv 2015 (later published at ICML 2015)

## 一、引言与核心问题

本论文探讨了机器学习中一个核心的挑战：如何构建既具有高度灵活性以捕捉复杂数据集内在结构，又保持计算和分析上的易处理性的概率模型。传统的概率模型常常面临这两者之间的权衡：易处理的模型（如高斯分布）往往表达能力不足，难以描述真实世界数据的丰富性；而灵活的模型则通常因为归一化常数难以计算，导致学习、采样和评估等过程代价高昂。这篇开创性的工作从非平衡态热力学中汲取灵感，提出了一种新的方法论，旨在同时实现模型的灵活性与易处理性。

*   **论文试图解决的核心任务是什么？**
    构建一种能够有效学习复杂数据分布，并且可以进行高效精确采样、概率评估和后验推断的深度无监督生成模型。

    *   **输入 (Input)**: 原始数据样本 $x^{(0)}$，来自于一个复杂的数据分布 $q(x^{(0)})$。这些数据可以是图像、序列或其他高维数据。
        *   **数据维度/Shape**: 对于通用数据，可以表示为 `[Batch_size, Data_dimension]`。例如，对于MNIST图像（假设展平），可能是 `[Batch_size, 784]`；对于CIFAR-10图像，可能是 `[Batch_size, Channels, Height, Width]` 即 `[Batch_size, 3, 32, 32]`. 论文中的 $x^{(t)}$ 通常指单个数据点，其维度与原始数据点 $x^{(0)}$ 相同。
    *   **输出 (Output)**: 一个学习到的生成模型 $p_\theta(x^{(0)})$，该模型能够:
        1.  从一个简单的先验分布（如高斯噪声）通过学习到的逆扩散过程生成新的数据样本 $x^{(0)}_{new}$，其维度与输入数据相同。
        2.  评估给定数据点 $x^{(0)}$ 的对数似然 $\log p_\theta(x^{(0)})$。
        3.  计算条件概率和后验概率，例如用于图像修复或去噪。
    *   **任务的应用场景**:
        *   图像生成与合成
        *   数据压缩
        *   图像去噪与修复 (Inpainting)
        *   无监督特征学习
        *   序列数据建模
    *   **当前任务的挑战 (Pain Points)**:
        1.  **灵活性与易处理性的冲突**: 简单的模型（如高斯混合模型）虽然易于处理，但无法捕捉高维数据的复杂依赖关系。
        2.  **归一化常数问题**: 许多灵活的能量模型或基于任意函数的模型 $p(x) = \frac{\phi(x)}{Z}$，其归一化常数 $Z = \int \phi(x) dx$ 通常是难以计算的 (intractable)，这使得模型的精确训练、采样和概率评估非常困难，往往需要昂贵的蒙特卡洛方法。
        3.  **深度生成模型的训练挑战**: 训练深度、多层的生成模型可能面临模式崩溃、训练不稳定或收敛缓慢等问题。
    *   **论文针对的难点**:
        该论文主要针对上述的**灵活性与易处理性的冲突**以及**归一化常数问题**。它提出了一种新的模型类别——扩散概率模型 (Diffusion Probabilistic Models)，通过一种迭代的、受控的噪声注入与去除过程来定义和学习模型，从而绕开了直接处理复杂归一化常数的难题。

## 二、核心思想与主要贡献

*   **直观动机与设计体现**:
    论文的直观动机源于非平衡态统计物理学。其核心思想是：
    
    1.  **前向扩散过程 (Forward Diffusion Process)**: 定义一个固定的马尔可夫链，它逐步地、缓慢地向数据中注入噪声，经过足够多的步骤后，原始数据分布 $q(x^{(0)})$ 会被平滑地转化为一个已知的、易于处理的简单分布 $\pi(x^{(T)})$（例如，标准高斯分布）。这个过程系统性地破坏数据的结构。
    2.  **反向扩散过程 (Reverse Diffusion Process)**: 学习一个马尔可夫链来逆转上述的扩散过程。这个反向过程从简单分布 $\pi(x^{(T)})$ 开始，逐步去除噪声，恢复数据的原始结构，从而定义了一个生成模型 $p_\theta(x^{(0)})$。
    这种方法的目的是同时实现模型的灵活性（因为理论上任何复杂分布都可以通过逐渐加噪变成简单分布，反之亦然）和易处理性（因为每一步的噪声添加和去除过程可以设计成数学上易于处理的形式）。
    
*   **与相关工作的比较与创新**:
    该工作与之前的生成模型如变分自编码器 (VAEs)、生成对抗网络 (GANs)、生成随机网络 (GSNs) 和自回归模型等相关。
    *   **VAEs**: VAEs也优化对数似然的下界，但扩散模型通过多步生成过程，可能捕捉更精细的数据结构。
    *   **GANs**: GANs通过判别器进行对抗训练，常能生成高质量样本但可能面临训练不稳定和模式覆盖不足的问题，且难以直接评估似然。扩散模型则直接优化似然（或其下界）。
    *   **GSNs**: GSNs也训练马尔可夫核以匹配数据分布，但本文提出的扩散过程具有更明确的物理学背景和特定的前向/反向结构。
    **创新之处在于**:
    1.  首次系统地将非平衡态热力学的思想引入深度无监督学习，构建了扩散概率模型。
    2.  模型定义为一个逐步去噪的马尔可夫链的终点，允许精确采样和（下界）似然评估。
    3.  能够训练具有数千层（时间步）的深度生成模型。
    4.  反向过程的每一步学习的是对前向噪声过程的小扰动的逆转，这比直接建模整个复杂分布更易处理。

*   **核心贡献与创新点**:
    1.  **提出了扩散概率模型 (Diffusion Probabilistic Models)**: 一种新的深度无监督学习框架，它通过模拟一个前向的结构破坏过程和一个学习到的反向结构恢复过程来生成数据。
    2.  **实现了灵活性与易处理性的统一**: 模型能够捕捉复杂数据分布（灵活性），同时允许精确采样、高效的对数似然（下界）评估以及条件概率计算（易处理性）。
    3.  **展示了模型的有效性**: 通过在多种数据集（包括玩具数据、图像和序列数据）上的实验，证明了模型能够生成高质量样本，并在对数似然方面取得有竞争力的结果。

**三、论文方法论 (The Proposed Pipeline)**

*   **整体架构概述**:
    该方法包含两个核心过程：一个固定的前向扩散过程 $q$ 和一个学习到的反向生成过程 $p_\theta$。
    1.  **前向过程 $q(x^{(1...T)}|x^{(0)})$ (Inference/Diffusion Process)**:
        这是一个固定的马尔可夫链，它从原始数据点 $x^{(0)} \sim q_{data}(x^{(0)})$ 开始，在 $T$ 个离散的时间步中逐渐添加高斯噪声。
        $q(x^{(t)}|x^{(t-1)}) = \mathcal{N}(x^{(t)}; x^{(t-1)}\sqrt{1-\beta_t}, I\beta_t)$
        其中 $\beta_t \in (0,1)$ 是一个预定义的噪声方差调度表，控制每一步添加噪声的量。当 $T \rightarrow \infty$ 且 $\beta_t$ 合适时，$x^{(T)}$ 近似服从标准高斯分布 $\mathcal{N}(0, I)$。
        令 $\alpha_t = 1-\beta_t$ 和 $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$，则 $x^{(t)}$ 可以直接从 $x^{(0)}$ 采样：
        $q(x^{(t)}|x^{(0)}) = \mathcal{N}(x^{(t)}; \sqrt{\bar{\alpha}_t}x^{(0)}, (1-\bar{\alpha}_t)I)$

    2.  **反向过程 $p_\theta(x^{(0...T)})$ (Generative Process)**:
        这是一个学习到的马尔可夫链，它从 $x^{(T)} \sim \mathcal{N}(0, I)$ 开始，目标是逆转前向过程，逐步去除噪声以生成数据样本 $x^{(0)}$。
        $p_\theta(x^{(0...T)}) = p(x^{(T)}) \prod_{t=1}^T p_\theta(x^{(t-1)}|x^{(t)})$
        其中 $p(x^{(T)}) = \mathcal{N}(x^{(T)}; 0, I)$。关键在于学习反向转移核 $p_\theta(x^{(t-1)}|x^{(t)})$。当 $\beta_t$ 足够小时，这个反向转移核也是高斯分布：
        $p_\theta(x^{(t-1)}|x^{(t)}) = \mathcal{N}(x^{(t-1)}; \mu_\theta(x^{(t)}, t), \Sigma_\theta(x^{(t)}, t))$
        其中均值 $\mu_\theta$ 和协方差 $\Sigma_\theta$（通常是对角阵或固定为与 $\beta_t$ 相关的量）由一个神经网络（参数为 $\theta$）根据当前噪声样本 $x^{(t)}$ 和时间步 $t$ 预测。

*   **详细网络架构与数据流**:
    *   **数据预处理**: 论文中提到，除二元心跳数据集外，其他数据集在计算对数似然之前都通过常数缩放使其方差为1。
    *   **网络 $f_\mu, f_\Sigma$ (或 $f_\theta$ for binomial)**: 神经网络用于参数化反向过程中的条件分布参数。输入是当前时间步的含噪数据 $x^{(t)}$ 和时间步索引 $t$（通常编码为一个特征向量）。
        *   对于瑞士卷和二元心跳等简单数据集，使用多层感知机 (MLP)。
        *   对于图像数据集（如CIFAR-10，Dead Leaves，Bark，MNIST），使用了基于卷积的架构，如附录D.2.1和图D.1所述：
            1.  **输入**: 当前噪声图像 $x^{(t)}$ (e.g., `[Channels, Height, Width]`) 和时间步 $t$。
            2.  **多尺度卷积 (Multi-Scale Convolution)**: 图像首先通过多尺度卷积层。这些层包括对图像进行均值池化下采样到多个尺度，在每个尺度上进行卷积，然后将结果上采样回原始分辨率并求和。这有助于捕捉不同尺度的特征。
            3.  **1x1 卷积层 (Dense per-pixel transformation)**: 之后通过若干1x1卷积核的卷积层，这等效于对每个像素的特征向量进行独立的密集变换。
                *   **形状变换**: 例如，`[Features_in, H, W]` -> `[Features_out, H, W]`.
            4.  **时间依赖性输出 (Temporal Dependence)**: 网络的输出 $y_i^\mu, y_i^\Sigma \in \mathbb{R}^{2J}$ (对于每个像素 $i$) 被用作时变“凸点函数 (bump functions)” $g_j(t)$ 的权重，从而生成与时间相关的输出 $z_i^\mu, z_i^\Sigma \in \mathbb{R}$ (公式62, 附录D.2.1)。
            5.  **均值和方差预测**: 最后，$z_i^\mu, z_i^\Sigma$ 与前向过程的 $\beta_t$ 结合，用于参数化反向高斯扩散核的均值 $\mu_i(x^{(t)},t)$ 和方差 $\Sigma_{ii}(x^{(t)},t)$ (公式64, 65, 附录D.2.1)。协方差通常被限制为对角阵 $\Sigma_\theta(x^{(t)}, t) = \sigma_t^2 I$，其中 $\sigma_t^2$ 可以是固定的（如 $\beta_t$ 或 $\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$）或由网络预测。
    *   **数据流**:
        1.  **训练时**:
            *   从数据集中采样 $x^{(0)}$。
            *   随机选择时间步 $t \in \{1, ..., T\}$。
            *   使用 $q(x^{(t)}|x^{(0)})$ 计算 $x^{(t)}$ (即加噪)。
            *   将 $x^{(t)}$ 和 $t$ 输入神经网络，预测 $p_\theta(x^{(t-1)}|x^{(t)})$ 的参数（例如，预测高斯均值，或等价地，预测添加到 $x^{(t)}$ 中的噪声 $\epsilon_\theta(x^{(t)},t)$）。
            *   计算损失函数并更新网络参数 $\theta$。
        2.  **采样时 (生成)**:
            *   从 $p(x^{(T)}) = \mathcal{N}(0, I)$ 采样 $x^{(T)}$。
            *   从 $t=T$ 迭代到 $t=1$:
                *   使用神经网络预测 $p_\theta(x^{(t-1)}|x^{(t)})$ 的参数。
                *   从该分布中采样 $x^{(t-1)}$。
            *   最终得到生成的样本 $x^{(0)}$。

*   **损失函数 (Loss Function)**:
    训练目标是最大化模型在数据上的对数似然 $L = \mathbb{E}_{q(x^{(0)})}[\log p_\theta(x^{(0)})]$。由于直接计算 $p_\theta(x^{(0)})$ 涉及对所有潜变量路径积分，通常是困难的。论文推导并优化其变分下界 (ELBO)，类似于VAE。
    $L \ge K = \mathbb{E}_{q(x^{(0...T)})} \left[ \log \frac{p_\theta(x^{(0...T)})}{q(x^{(1...T)}|x^{(0)})} \right]$
    这个下界可以被重写为多个项的和（如公式14，以及更详细的推导见附录B，最终形式如公式51）：
    $K = - \sum_{t=2}^{T} \mathbb{E}_{q(x^{(0)},x^{(t)})} [D_{KL}(q(x^{(t-1)}|x^{(t)}, x^{(0)}) || p_\theta(x^{(t-1)}|x^{(t)}))] + H_q(X^{(T)}|X^{(0)}) - H_q(X^{(1)}|X^{(0)}) - H_p(X^{(T)})$
    *   **设计理念**: 该损失函数鼓励学习到的反向条件分布 $p_\theta(x^{(t-1)}|x^{(t)})$ 去匹配真实的前向过程后验 $q(x^{(t-1)}|x^{(t)}, x^{(0)})$。$H_q$ 和 $H_p$ 是与已知分布相关的熵项，不依赖于 $\theta$。
    *   **关注重点**: 核心是最小化一系列KL散度项，每一项对应一个时间步的反向预测。这等价于让模型在每一步都能准确地“去噪”。
    *   **训练实施**: 通过从数据中采样 $x^{(0)}$，然后采样一个时间步 $t$ 和对应的 $x^{(t)}$，再使用蒙特卡洛方法估计KL散度项的期望，并使用梯度下降进行优化。
    *   **对性能的贡献**: 这种损失函数的设计允许模型以一种分解的方式学习复杂的分布，每一步都相对简单，从而使得整个学习过程更稳定和有效。论文中的实验结果表明，该方法能够获得较好的对数似然下界。

*   **数据集 (Dataset)**:
    *   **所用数据集**:
        *   **玩具数据**: 2D 瑞士卷 (Swiss Roll), 二元心跳序列 (Binary Heartbeat)。
        *   **自然图像纹理**: 巴克图像 (Bark), 落叶图像 (Dead Leaves)。
        *   **标准图像数据集**: CIFAR-10, MNIST 手写数字。
    *   **特殊处理**:
        *   大多数连续数据集在计算对数似然前被缩放以使方差为1。
        *   对于MNIST的对数似然评估，使用了(Goodfellow et al., 2014)的Parzen窗代码进行估计，以与先前工作对比。
        *   CIFAR-10 (脚注3, p7): 用于最终报告的对数似然界限的数据经过预处理，通过添加均匀噪声来消除像素量化效应，遵循(Theis et al., 2015)的建议。
        *   前向扩散率 $\beta_t$ (噪声调度)：对于高斯扩散，$\beta_t$ 可以通过在 $K$ 上进行梯度上升来学习（第2.4.1节）。对于二项式扩散，则选择固定的调度 $\beta_t = (T-t+1)^{-1}$。

**四、实验结果与分析**

*   **核心实验结果**:
    论文通过在多个数据集上评估对数似然下界 $K$ (bits/dim 或 bits/seq) 来展示模型性能。
    *   **表1**: 展示了在瑞士卷、二元心跳、巴克、落叶、CIFAR-10和MNIST上的 $K$ 值及其相对于简单基线 ($L_{null}$) 的提升。
        *   例如，在Dead Leaves上达到1.489 bits/pixel，比 $L_{null}$ 提升3.536 bits/pixel。
        *   MNIST上达到 $317 \pm 2.7$ bits (总和，不是bits/pixel)。
    *   **表2**: 将模型在Dead Leaves和MNIST上的对数似然结果与其他算法进行了比较。
        *   **Dead Leaves**: 扩散模型 (1.489 bits/pixel) 优于 MCGSM (1.244 bits/pixel)。
        *   **MNIST**: 扩散模型 ($317 \pm 2.7$ bits) 与当时的先进方法（如深度GSN $309 \pm 1.6$ bits, 对抗网络 $325 \pm 2.9$ bits）表现相当或更好。

        | 数据集     | 模型                      | 对数似然 (bits/pixel or total bits) |
        |------------|---------------------------|---------------------------------------|
        | Dead Leaves| MCGSM                     | 1.244                                 |
        | Dead Leaves| Diffusion (本文)          | **1.489**                             |
        | MNIST      | Stacked CAE               | 174 ± 2.3                             |
        | MNIST      | DBN                       | 199 ± 2.9                             |
        | MNIST      | Deep GSN                  | 309 ± 1.6                             |
        | MNIST      | Diffusion (本文)          | **317 ± 2.7**                         |
        | MNIST      | Adversarial net           | 325 ± 2.9                             |
        | MNIST      | Perfect model (Parzen on train data) | 349 ± 3.3                             |

*   **消融研究解读**:
    论文虽然没有明确的“消融研究”表格，但在方法描述中讨论了不同设计选择的影响：
    *   **扩散率 $\beta_t$ 的学习**: 第2.4.1节提到，对于高斯扩散，学习 $\beta_t$ 是有益的。但脚注2 (p6) 指出，后来的实验表明使用固定的 $\beta_t$ 调度（同二项式扩散）也同样有效。
    *   **模型架构**: 附录D中描述的针对图像的多尺度卷积架构和时间依赖性读出机制是为了更好地捕捉图像的复杂结构和时间（扩散步）演化。
*   **可视化结果分析**:
    *   **图1 (瑞士卷)**: 清晰展示了前向扩散过程如何将数据变为高斯噪声，以及学习到的反向过程如何从噪声中恢复出瑞士卷结构，证明了模型在低维流形上的学习能力。
    *   **图2 (二元心跳)**: 生成的二元序列与训练数据几乎完全一致，表明模型能精确学习离散序列的确定性结构。
    *   **图3 (CIFAR-10)**: (a) 原始数据, (b) 加噪数据, (c) 从加噪数据条件生成的去噪图像, (d) 无条件生成的样本。展示了模型的去噪能力和样本生成质量。
    *   **图4 (落叶图像)**: (c) 本文模型生成的样本与 (b) 当时SOTA模型 (Theis et al., 2012) 的样本对比。扩散模型生成的样本展现了良好的一致性遮挡关系、多尺度的物体分布和更清晰的圆形物体，尤其是在小尺度上。
    *   **图5 (巴克图像修复)**: 展示了模型在条件生成任务（图像修复）上的应用。模型能够根据图像的已知部分，在缺失区域（中央100x100像素）生成具有长程空间结构的合理内容（如裂纹的延续）。

**五、方法优势与深层分析**

*   **架构/设计优势**:
    *   **优势详述**:
        1.  **易处理的条件分布**: 核心优势在于将复杂的联合分布建模 $p(x^{(0)})$ 分解为一系列易于处理的条件分布 $p_\theta(x^{(t-1)}|x^{(t)})$ 的学习。由于每一步的噪声扰动 $\beta_t$ 很小，这些条件分布通常可以被参数化为简单的形式（如高斯分布），其参数由神经网络预测。
        2.  **稳定的训练目标**: 优化目标是最小化一系列KL散度，这通常比GANs的对抗性训练目标更稳定，并且提供了对模型拟合优度的直接度量（对数似然下界）。
        3.  **精确采样**: 一旦模型训练完成，可以通过从先验 $p(x^{(T)})$ 开始，迭代执行学习到的反向马尔可夫转移进行精确采样。
        4.  **灵活的模型容量**: 可以通过增加扩散步数 $T$ 或使用更强大的神经网络（如深度卷积网络）来增强模型的表达能力，以适应更复杂的数据。
        5.  **无需处理全局归一化常数**: 模型的概率是通过一系列局部归一化的条件分布定义的，避免了直接计算整个模型 $p(x^{(0)})$ 的难以处理的归一化常数 $Z$。
    *   **原理阐释**:
        *   **分解原理**: 基于马尔可夫链的性质，联合概率可以分解为初始分布和一系列转移概率的乘积。扩散模型巧妙地利用了这一点，将生成过程视为一个反向的马尔可夫链。
        *   **小扰动原理**: 在扩散的每一步，如果噪声扰动足够小，那么反向过程的条件分布与前向过程的条件分布具有相同的函数形式（例如，高斯到高斯）。这使得参数化和学习反向过程成为可能。
        *   **变分推断**: 训练过程本质上是变分推断，通过最大化对数似然的下界来学习模型参数，其中前向过程充当了变分后验的角色（尽管是固定的）。

*   **解决难点的思想与实践**:
    *   **核心思想**: 通过“逐步破坏，逐步恢复”的思想来解决直接建模复杂分布的难题。不是一步到位地从噪声生成数据，而是通过一个多阶段的、细致的去噪过程。
    *   **实践手段**:
        1.  **定义固定的前向噪声过程**: 提供了一个从数据到已知噪声分布的桥梁。
        2.  **参数化反向去噪过程**: 使用神经网络学习每一步如何从 $x^{(t)}$ 预测 $x^{(t-1)}$（或等价地，预测噪声）。
        3.  **设计可优化的损失函数**: 基于KL散度构建损失函数，使得模型学习去匹配真实的前向过程的逆过程。
        4.  **迭代采样机制**: 实现了从简单噪声分布生成复杂数据样本。

**六、结论与个人思考**

*   **论文的主要结论回顾**:
    论文成功引入了一种新的深度无监督学习算法——扩散概率模型，该模型基于非平衡态热力学的思想。它能够学习复杂数据的分布，实现精确采样和概率评估（或其下界），并在多个数据集上展示了其有效性。核心机制在于估计一个马尔可夫扩散链（将数据映射到噪声）的逆过程。

*   **潜在局限性 (基于2015年视角)**:
    1.  **采样速度**: 由于生成一个样本需要执行 $T$ 次（$T$ 通常较大，如1000或数千）神经网络的前向传播，采样速度相对较慢。
    2.  **对数似然下界**: 虽然可以评估对数似然的下界 $K$，但这个下界可能不够紧，尤其是在模型与真实后验差异较大时。
    3.  **噪声调度$\beta_t$的选择**: $\beta_t$ 的选择对模型性能至关重要，可能需要仔细调整或额外的学习过程，论文中也提到了学习$\beta_t$的可能性。
    4.  **模型复杂性**: 尽管每一步简单，但整个模型涉及大量时间步和参数，训练和存储可能需要较多资源。

*   **未来工作方向 (基于2015年视角，很多已成为现实)**:
    1.  **加速采样过程**: 研究如何在不显著牺牲样本质量的前提下减少采样所需的步数。
    2.  **更紧的对数似然界**: 探索新的目标函数或训练策略以获得更紧的对数似然估计。
    3.  **探索不同的扩散核和噪声类型**: 研究非高斯噪声或更复杂的扩散过程。
    4.  **条件生成**: 进一步发展和应用扩散模型于条件生成任务，如文本到图像、图像超分辨率等。
    5.  **理论理解**: 深化对扩散模型数学性质和学习动态的理论分析。

*   **对个人研究的启发**:
    这篇论文是扩散模型领域的奠基之作，展示了如何从物理过程中汲取灵感来设计强大的生成模型。它强调了将复杂问题分解为一系列简单步骤的有效性，并为后续大量的扩散模型研究奠定了基础。其核心的逐步去噪思想具有普适性，可能启发其他领域中类似的多阶段建模方法。

**七、代码参考与分析建议**

*   **仓库链接**: `https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models` (论文第8页提及)
*   **核心模块实现探讨**:
    建议读者查阅作者提供的代码（如果仍然可访问和可运行），重点关注以下模块的实现，以理解其具体工作方式和参数配置：
    1.  **前向扩散过程 (`q_sample` 或类似函数)**: 如何根据 $x^{(0)}$ 和时间步 $t$ 以及噪声调度 $\beta_t$ (或 $\bar{\alpha}_t$) 来生成带噪样本 $x^{(t)}$。
    2.  **反向去噪网络 (`model` 或 `unet` 等)**: 论文附录D.2.1中描述的针对图像的多尺度卷积网络架构的实现。特别注意其输入 ($x^{(t)}$, $t$) 和输出（用于参数化 $p_\theta(x^{(t-1)}|x^{(t)})$ 的均值和方差，或直接预测噪声）。
    3.  **时间步编码/嵌入**: 如何将离散的时间步 $t$ 转化为网络可以处理的特征向量。
    4.  **损失函数计算 (`loss` 函数)**: 如何实现基于KL散度的损失函数，特别是如何计算 $q(x^{(t-1)}|x^{(t)}, x^{(0)})$ 的参数和 $p_\theta(x^{(t-1)}|x^{(t)})$ 的参数，并计算它们之间的KL散度。
    5.  **采样过程 (`p_sample_loop` 或类似函数)**: 如何从 $x^{(T)}$ 开始，迭代地使用训练好的网络进行采样以生成 $x^{(0)}$。
    6.  **噪声调度表 ($\beta_t$)**: 如何定义和使用噪声调度表。

通过分析这些核心模块，可以更深入地理解扩散概率模型从理论到实践的具体实现细节。