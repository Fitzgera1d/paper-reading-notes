---

**论文标题: iBOT: Image BERT Pre-Training with Online Tokenizer - ICLR 2022**

**一、引言与核心问题**

自注意力机制在自然语言处理 (NLP) 领域取得了巨大成功，特别是以 BERT (Bidirectional Encoder Representations from Transformers) 为代表的预训练模型，通过掩码语言建模 (Masked Language Modeling, MLM) 等自监督任务，从大规模无标注文本中学习到了丰富的语言表示。受到 NLP 成功的启发，研究者们开始探索将类似的思想应用于计算机视觉领域，期望通过自监督学习从海量无标注图像中学习通用的视觉表示，从而惠及各种下游视觉任务。iBOT (Image BERT) 正是这一趋势下的重要尝试，它旨在将 BERT 的成功经验更直接地迁移到视觉领域，通过引入在线分词器 (Online Tokenizer) 等创新设计，有效地解决了图像数据与文本数据在处理上的差异性，从而实现了高效的图像 BERT 预训练。

*   **论文试图解决的核心任务是什么？**
    该论文的核心任务是进行图像的自监督预训练，学习通用的视觉表示，以便能够迁移到各种下游视觉任务并取得优异性能。具体来说，它旨在通过类似 BERT 的掩码图像建模 (Masked Image Modeling, MIM) 方式，让模型从未标注的图像数据中学习到丰富的语义和上下文信息。

    *   **输入 (Input)**:
        *   **类型和形态**: 输入主要是无标注的图像数据。在预训练阶段，通常使用标准的图像数据集，如 ImageNet-1K。
        *   **数据维度/Shape**: 对于标准的图像输入，其维度通常为 `[Batch_size, Channels, Height, Width]`，例如 `[B, 3, 224, 224]`。论文中，图像首先被分割成一系列不重叠的图像块 (patches)。例如，一个 `224x224` 的图像，如果块大小为 `16x16`，则会产生 `(224/16) * (224/16) = 14 * 14 = 196` 个图像块。每个图像块会被线性投影为一个特征向量。
        *   **特殊之处**: iBOT 的一个关键设计在于其处理输入的方式。它采用了类似 ViT (Vision Transformer) 的图像分块策略，将图像视为一系列"视觉词元 (visual tokens)"。与早期 ViT 直接将图像块线性嵌入不同，iBOT 在掩码图像建模任务中，对被掩码的图像块，其目标是预测这些块的离散视觉词元，而这些词元是由一个在线更新的教师网络 (teacher network) 的输出来定义的。

    *   **输出 (Output)**:
        *   **类型和形态**: 预训练阶段的直接输出是模型学习到的图像表示（特征向量）。对于掩码图像建模任务，模型需要预测被掩码图像块的视觉词元。这些词元是离散的，由在线分词器（教师网络）动态生成。
        *   **数据维度/Shape**: 经过 Transformer 编码器后，每个图像块都会有一个对应的输出特征向量。如果输入有 `N` 个块，每个块的特征维度是 `D`，那么输出可以看作是 `[Batch_size, N, D]`。在进行下游任务微调时，通常会取 `[CLS]` 词元（如果使用的话）的输出特征，或者对所有块的特征进行平均池化，得到一个全局图像表示，其维度为 `[Batch_size, D]`。

    *   **任务的应用场景**:
        学习到的通用视觉表示可以广泛应用于各种下游计算机视觉任务，包括但不限于：
        *   图像分类 (Image Classification)
        *   目标检测 (Object Detection)
        *   语义分割 (Semantic Segmentation)
        *   实例分割 (Instance Segmentation)
        *   姿态估计 (Pose Estimation)
        *   图像生成 (Image Generation，虽然 iBOT 主要关注表示学习)

    *   **当前任务的挑战 (Pain Points)**:
        在 iBOT 提出之前，视觉自监督学习，特别是基于 Transformer 的方法，面临一些挑战：
        *   **视觉词元的定义**: 与 NLP 中天然存在的离散词汇不同，图像是连续信号，如何定义"视觉词元"并进行有效的掩码和预测是一个核心问题。早期的 MIM 方法通常预测原始像素值或手工设计的特征 (如 HOG)，这可能导致模型过于关注低级细节或引入不必要的先验。
        *   **表示坍塌 (Representation Collapse)**: 如果目标过于简单或不合适，模型可能学习到平凡解，导致表示缺乏区分度。
        *   **计算效率与训练稳定性**: Transformer 模型通常参数量大，训练需要大量数据和计算资源。如何设计高效的预训练任务和稳定的训练策略至关重要。
        *   **与 NLP 的差异**: 直接套用 NLP 的掩码语言建模可能不适用于视觉领域，因为图像的空间冗余性远高于文本，随机掩码大量块可能使得任务过于简单。

    *   **论文针对的难点**:
        iBOT 主要针对以下难点进行设计和改进：
        *   **定义高质量的视觉词元**: 通过引入一个与学生网络 (student network) 具有相同架构但参数通过指数移动平均 (EMA) 更新的教师网络 (teacher network)，iBOT 动态地生成高质量的离散视觉词元作为掩码预测的目标。这避免了使用固定的、预定义的词汇表或直接预测原始像素。
        *   **防止表示坍塌并学习高级语义**: 通过将学生网络的输出与教师网络生成的在线词元进行对齐，并使用交叉熵损失进行优化，iBOT 鼓励学生网络学习到与教师网络一致的、更抽象和语义丰富的表示。教师网络的EMA更新机制保证了其输出的稳定性和一致性。
        *   **有效的掩码策略**: iBOT 采用了块级掩码 (block-wise masking) 策略，并探索了不同的掩码率，以平衡任务难度和学习效率。

**二、论文方法论 (The Proposed Pipeline)**

*   **整体架构概述**:
    iBOT 的核心思想是构建一个自监督学习框架，其中包含一个学生 (student) 网络和一个教师 (teacher) 网络，两者具有相同的 Vision Transformer (ViT) 架构。学生网络通过掩码图像建模 (MIM) 任务进行训练，即预测被随机掩码的图像块的视觉词元。关键在于，这些目标视觉词元并非预先固定的，而是由教师网络在线生成的。教师网络的参数是学生网络参数的指数移动平均 (EMA)，这使得教师网络能够提供稳定且不断演进的优质目标。此外，iBOT 还引入了自蒸馏机制，让学生网络的 `[CLS]` 词元输出去匹配教师网络的 `[CLS]` 词元输出，进一步增强了全局语义表示的学习。这种设计使得 iBOT 能够从未标注图像中学习到强大的视觉表示。

*   **详细网络架构与数据流**:
    1.  **数据预处理与分块**:
        *   输入图像首先进行数据增强，例如随机裁剪、颜色抖动等。
        *   增强后的图像被分割成一系列不重叠的图像块 (patches)，例如 `16x16` 像素。这些块按光栅扫描顺序排列。

    2.  **图像块掩码**:
        *   一部分图像块被随机选择并进行掩码。iBOT 采用了块级掩码策略，即一次掩码掉一个较大的连续图像块区域，而不是随机独立地掩码小块。这种策略被认为更适合视觉任务，因为它能更好地模拟物体遮挡等真实场景。

    3.  **学生网络 (Student Network)**:
        *   **输入**: 所有图像块（包括未掩码和已掩码的）经过线性投影层，转换为特征向量序列，并加入位置编码 (positional embeddings)。一个可学习的 `[CLS]` 词元也被添加到序列的开头。
        *   **网络结构**: 学生网络采用标准的 Vision Transformer (ViT) 编码器结构，由多层 Transformer Block 组成。每个 Transformer Block 包含一个多头自注意力 (Multi-Head Self-Attention, MHSA) 模块和一个前馈网络 (Feed-Forward Network, FFN)。层归一化 (Layer Normalization, LN) 应用于每个模块之前，残差连接 (Residual Connection) 也被使用。
        *   **形状变换**:
            *   输入图像 `[B, C, H, W]` -> 分块后 `[B, Num_patches, Patch_dim]`。
            *   线性投影后 `[B, Num_patches, Embed_dim]`。
            *   加入 `[CLS]` 词元后 `[B, Num_patches + 1, Embed_dim]`。
            *   经过 ViT 编码器后，输出形状仍为 `[B, Num_patches + 1, Embed_dim]`。
        *   **输出**: 学生网络对每个输入的图像块（包括被掩码的块）都会输出一个特征表示。对于被掩码的块，其输出特征将用于预测该块的视觉词元。`[CLS]` 词元的输出特征则用于全局图像表示的学习。

    4.  **教师网络 (Teacher Network) 与在线分词器**:
        *   **架构**: 教师网络与学生网络具有完全相同的 ViT 架构。
        *   **参数更新**: 教师网络的参数 $ \theta_t $ 不是通过梯度下降直接优化的，而是学生网络参数 $ \theta_s $ 的指数移动平均 (EMA): $ \theta_t \leftarrow \lambda \theta_t + (1-\lambda) \theta_s $，其中 $ \lambda $ 是一个平滑系数，通常接近1。这种更新方式使得教师网络的变化比学生网络更平滑，能提供更稳定的目标。
        *   **在线生成视觉词元**: 对于同一张输入图像（但可能施加了不同的数据增强，例如 DINO 中的多裁剪策略），教师网络在不进行掩码的情况下处理它，并为每个图像块生成特征表示。这些特征表示随后通过一个共享的投影头 (projection head，通常是一个 MLP) 映射到一个概率分布空间，然后通过 `softmax` 函数得到每个块属于各个预定义词元类别（原型）的概率。这个过程可以看作是一个"在线分词器"，它将连续的图像块特征映射为离散的词元概率分布。
        *   **目标生成**: 对于学生网络中被掩码的图像块，其预测目标就是教师网络对应块输出的词元概率分布。

    5.  **数据流总结**:
        *   原始图像 $x$。
        *   对 $x$ 进行不同的数据增强，得到学生视图 $x_s$ 和教师视图 $x_t$。
        *   $x_s$ 被分块并进行掩码，输入学生网络 $f_s(\cdot)$。
        *   $x_t$ 被分块（不掩码），输入教师网络 $f_t(\cdot)$。
        *   学生网络对掩码块的输出 $f_s(x_s)_{masked}$ 用于预测由教师网络生成的对应块的视觉词元 $f_t(x_t)_{unmasked}$。
        *   学生网络的 `[CLS]` 词元输出 $f_s(x_s)_{[CLS]}$ 与教师网络的 `[CLS]` 词元输出 $f_t(x_t)_{[CLS]}$ 对齐。

    *   **结合消融实验的作用分析**:
        论文中的消融实验证明了各个关键组件的有效性：
        *   **在线分词器 (Online Tokenizer)**: 与使用固定词汇表或预测原始像素值相比，使用在线教师网络生成动态目标词元显著提升了性能。这表明高质量、动态更新的语义目标对于学习强表示至关重要。
        *   **掩码策略**: 块级掩码通常比随机独立掩码效果更好。
        *   **自蒸馏 (`[CLS]` 词元对齐)**: 将学生网络的 `[CLS]` 输出与教师网络的 `[CLS]` 输出对齐，有助于学习更好的全局图像表示，对图像分类等任务有益。
        *   **教师网络 EMA 更新**: EMA 更新策略对于教师网络的稳定性至关重要，从而为学生网络提供了一致的学习信号。

*   **损失函数 (Loss Function)**:
    iBOT 的损失函数主要包含两部分：

    1.  **掩码图像建模损失 (Masked Image Modeling Loss)**:
        *   **设计理念**: 对于学生网络中被掩码的图像块，其目标是预测教师网络对应未掩码块输出的视觉词元（概率分布）。这通过计算学生网络输出与教师网络输出之间的交叉熵损失来实现。
        *   **数学形式**: 设 $P_s(x_s^m)$ 是学生网络对第 $m$ 个掩码块的预测概率分布（经过投影头和 softmax），$P_t(x_t^m)$ 是教师网络对同一位置（但来自教师视图 $x_t$）未掩码块生成的概率分布（同样经过投影头和 softmax，并进行 sharpen 以产生更尖锐的分布作为目标）。MIM 损失可以表示为：
            $$ L_{MIM} = - \sum_{m \in M} P_t(x_t^m) \log P_s(x_s^m) $$
            其中 $M$ 是被掩码块的集合。教师网络的输出 $P_t(x_t^m)$ 通常会经过一个 temperature sharpening 操作，使其分布更接近 one-hot，作为伪标签。
        *   **关注重点**: 此损失函数关注局部图像块的语义理解和重建能力，鼓励模型学习到块级别的细粒度特征。

    2.  **自蒸馏损失 (Self-Distillation Loss on `[CLS]` token)**:
        *   **设计理念**: 类似于 DINO，iBOT 也让学生网络的 `[CLS]` 词元输出去匹配教师网络的 `[CLS]` 词元输出。这有助于学生网络学习到更好的全局图像表示。
        *   **数学形式**: 设 $P_s(x_s^{[CLS]})$ 和 $P_t(x_t^{[CLS]})$ 分别是学生和教师网络 `[CLS]` 词元经过投影头和 softmax 后的输出概率分布。自蒸馏损失同样使用交叉熵：
            $$ L_{CLS} = - P_t(x_t^{[CLS]}) \log P_s(x_s^{[CLS]}) $$
            同样，教师网络的 `[CLS]` 输出也会经过 sharpen 处理。
        *   **关注重点**: 此损失函数关注全局图像级别的语义表示学习。

    *   **总损失**: 最终的总损失是这两部分损失的加权和：
        $$ L_{total} = L_{MIM} + \alpha L_{CLS} $$
        其中 $ \alpha $ 是一个平衡两者的超参数。

    *   **训练实施**:
        *   教师网络的参数通过学生网络参数的 EMA 进行更新，不参与反向传播。
        *   只有学生网络的参数通过梯度下降进行优化。
        *   使用了诸如权重衰减、学习率调度（如 cosine decay）、AdamW 优化器等标准训练技巧。
        *   通常采用多裁剪 (multi-crop) 策略，即为同一张图像生成多个不同分辨率和大小的视图（一些全局视图，一些局部视图），学生网络处理所有视图，而教师网络可能只处理全局视图，以生成更一致的目标。

    *   **效果评估**: 论文通过在多个下游任务（如 ImageNet-1K 线性探测、k-NN 分类、目标检测、语义分割）上的性能来评估损失函数设计的有效性的。与之前的自监督方法相比，iBOT 取得了显著的性能提升，证明了其在线分词器和联合 MIM 与自蒸馏框架的有效性。特别是，在线生成的视觉词元被认为是其成功的关键因素之一，因为它避免了设计固定词汇表的困难，并能适应数据分布的变化。

*   **数据集 (Dataset)**:
    *   **所用数据集**:
        *   **预训练**: 主要在 ImageNet-1K (IN-1K) 无标签数据集上进行预训练。ImageNet-1K 包含约 128 万张训练图像，分布在 1000 个类别中（但预训练时通常不使用标签）。
        *   **下游任务评估**:
            *   **图像分类**: ImageNet-1K (带标签，用于线性探测或全网络微调), iNaturalist, Places205 等。
            *   **目标检测与实例分割**: COCO 数据集。
            *   **语义分割**: ADE20K, Pascal VOC 2012 数据集。
    *   **特殊处理**:
        *   **数据增强**: 采用了标准且强效的数据增强方法，包括随机裁剪、颜色抖动、高斯模糊、Solarization 等。多裁剪策略（multi-crop training，如 DINO 中使用的）也被广泛应用，即从一张图像中生成多个不同大小和视角的裁剪（通常是两个全局视图和多个局部视图）。学生网络看到所有裁剪，而教师网络通常只看到全局裁剪以提供稳定的目标。
        *   **无特殊筛选或构建**: 论文主要依赖于标准的大规模图像数据集进行预训练，未提及对数据集进行特殊的筛选或构建过程。其核心贡献在于预训练方法本身，而非数据集的特殊处理。数据增强是保证模型泛化能力和学习鲁棒表示的关键。

**三、实验结果与分析**

*   **核心实验结果**:
    iBOT 在多个基准测试中均取得了当时最先进 (SOTA) 或具有竞争力的结果，显著优于之前的自监督学习方法，包括 MoCo v3, DINO, EsViT 等。
    *   **ImageNet-1K k-NN 分类**: iBOT 在 ImageNet-1K 上的 k-NN 分类任务中表现出色，这表明其学习到的特征具有很好的区分性，即使不进行微调也能捕捉到类别间的相似性。
    *   **ImageNet-1K 线性探测 (Linear Probing)**: 在冻结预训练的骨干网络，只训练一个线性分类器的情况下，iBOT 取得了非常高的准确率，证明了其学习到的表示的线性可分性强。
    *   **ImageNet-1K 微调 (Fine-tuning)**: 对整个网络进行微调时，iBOT 同样展现了强大的性能，超过了许多有监督预训练的模型。
    *   **迁移学习性能**: 在下游任务如 COCO 目标检测与实例分割、ADE20K 语义分割上，使用 iBOT 预训练的骨干网络进行微调也取得了显著的性能提升。

*   **消融研究解读**:
    论文进行了详细的消融实验，以验证 iBOT 各个设计组件的有效性：
    *   **在线分词器的重要性**: 实验对比了使用在线教师网络生成目标词元与使用固定 tokenizer (如 dVAE 或 k-means 聚类生成的离散词汇表) 的效果。结果表明，在线分词器能够显著提升性能，因为它能动态适应数据特征并提供更高质量的语义目标。
    *   **掩码预测任务 (MIM)**: 单独的 MIM 任务是 iBOT 的核心。实验证明，预测由教师网络生成的语义词元比预测原始像素或 HOG 特征更有效。
    *   **`[CLS]` 词元自蒸馏**: 在 MIM 的基础上加入 `[CLS]` 词元的自蒸馏，进一步提升了全局表示的质量，尤其有利于线性评估和图像分类任务。
    *   **教师网络 EMA**: 调整 EMA 的平滑系数 $ \lambda $ 对性能有影响，较高的 $ \lambda $ (如 0.996 到 0.9999) 通常能带来更稳定的训练和更好的结果。
    *   **不同的掩码策略**: 论文对比了不同的掩码策略（例如，随机掩码独立的小块 vs. 掩码较大的连续块）。块级掩码被证明更有效，因为它更能模拟真实世界中的遮挡，并迫使模型理解上下文信息。
    *   **多裁剪训练**: 使用多裁剪策略显著增强了模型的性能和鲁棒性。

*   **可视化结果分析**:
    论文通常会提供以下类型的可视化结果来证明其方法的有效性：
    *   **注意力图 (Attention Maps)**: 可视化 ViT 编码器最后一层的自注意力图，可以观察到模型是否能够自动关注到图像中的显著物体或区域，即使在没有显式监督的情况下。iBOT 的注意力图通常能清晰地勾勒出物体轮廓，表明其学习到了良好的空间定位和语义分割能力。
    *   **最近邻检索**: 给定一个查询图像块，在特征空间中检索其最近邻的图像块。如果检索到的块在语义上与查询块相似，则表明模型学习到了有意义的语义表示。
    *   **特征空间可视化 (如 t-SNE)**: 将学习到的图像特征降维到二维或三维空间进行可视化。理想情况下，来自同一语义类别的图像特征应该聚集在一起，而不同类别的特征则相互分离。iBOT 的特征空间通常展现出良好的聚类结构。
    *   **掩码重建可视化**: 虽然 iBOT 不直接重建像素，但可以可视化模型对掩码块预测的"最可能"的视觉词元所对应的原型图像块（如果教师网络生成的是离散原型的话），或者通过某种方式将预测的语义词元解码回图像空间。这能直观展示模型对被遮挡内容的理解程度。

**四、论文的创新点、优势及其成因**

*   **核心创新点**:
    1.  **在线分词器 (Online Tokenizer)**: 这是 iBOT 最核心的创新。通过引入一个与学生网络架构相同、参数由学生网络 EMA 更新的教师网络，iBOT 能够动态地为掩码图像块生成高质量、语义丰富的在线目标词元。这避免了依赖固定、预训练的离散视觉词汇表 (如 dVAE) 或预测低级像素信息，使得模型能够学习更抽象和鲁棒的表示。
    2.  **联合掩码图像建模与自蒸馏**: iBOT 将掩码图像建模 (MIM) 任务与 `[CLS]` 词元的自蒸馏机制相结合。MIM 关注局部块的语义预测，而 `[CLS]` 词元的对齐则促进了全局图像表示的学习。这种联合学习框架使得模型能够同时捕获局部细节和全局语义。
    3.  **统一的自监督学习框架**: iBOT 成功地将 BERT 的掩码预测思想与对比学习中的教师-学生蒸馏机制 (如 DINO) 相结合，为视觉自监督学习提供了一个更统一和强大的框架。

*   **架构/设计优势**:
    *   **优势详述与原理阐释**:
        *   **高质量的自监督信号**: 在线分词器提供的目标是动态的、语义级别的，并且由一个更"稳定"和"先进"（通过EMA更新）的教师网络产生。这比预测固定目标（如像素值或预定义词元）能引导学生网络学习到更深层次的语义信息。教师网络的EMA更新确保了目标的稳定性和一致性，避免了学生网络追逐一个快速变化或充满噪声的目标，从而促进了稳定的收敛和高质量表示的学习。
        *   **端到端的表示学习**: 整个过程是端到端学习的，视觉词元的定义和预测任务都融入到同一个优化过程中，使得词元本身就是为了优化最终的表示学习目标而生成的。
        *   **对 ViT 架构的良好适应性**: iBOT 的设计与 Vision Transformer 架构天然契合。ViT 将图像视为序列的处理方式使得掩码和预测特定"词元"（图像块）变得直接。
        *   **减少对特定解码器的依赖**: 与一些需要复杂解码器将潜在表示映射回像素空间的方法不同，iBOT 的 MIM 任务是在特征/词元级别进行的，简化了设计并可能更关注语义。
        *   **强大的泛化能力**: 通过在大规模无标注数据上进行预训练，iBOT 学习到的表示具有很强的泛化能力，能够有效地迁移到各种下游任务并取得优异性能。

*   **解决难点的思想与实践**:
    iBOT 通过以下核心思想和实践有效解决了视觉自监督学习中的难点：
    *   **针对"视觉词元定义"的难点**:
        *   **思想**: 放弃预定义或固定词汇表的思路，转而通过一个动态的、与学习过程耦合的机制来在线生成语义目标。
        *   **实践**: 引入教师网络作为"在线分词器"。教师网络处理未掩码的图像视图，其输出（经过投影头和softmax）为每个图像块生成一个概率分布，这个分布就充当了学生网络预测的目标"视觉词元"。教师网络的EMA更新保证了这些词元的质量和稳定性。
    *   **针对"表示坍塌"和"学习高级语义"的难点**:
        *   **思想**: 结合掩码预测的局部判别和全局表示的一致性约束，同时利用教师-学生架构的知识蒸馏优势。
        *   **实践**:
            1.  **MIM 损失**: 要求学生网络预测教师网络生成的、经过 sharpen 的语义词元分布，这本身就是一个具有挑战性的判别任务，迫使学生网络学习细粒度的特征。
            2.  **`[CLS]` 词元自蒸馏**: 强制学生网络的全局 `[CLS]` 表示与教师网络的全局 `[CLS]` 表示对齐，这借鉴了 DINO 等对比学习方法的思想，有助于学习区分性强的全局特征，并防止模型坍塌到平凡解。
            3.  **教师网络的中心化 (Centering) 和锐化 (Sharpening)**: 对教师网络的输出进行中心化和锐化操作，可以防止某一类别主导输出，并产生更清晰的目标分布，进一步避免坍塌并提升学习效率。
    *   **针对"有效的掩码策略"**:
        *   **思想**: 采用更符合视觉数据特性的掩码方式，模拟真实遮挡。
        *   **实践**: 采用块级掩码 (block-wise masking) 策略，并对不同的掩码率进行了实验探索，找到了一个较优的平衡点。

**五、结论与个人思考**

iBOT 论文提出了一种新颖且高效的图像 BERT 预训练方法，其核心贡献在于引入了在线分词器机制。通过一个与学生网络共享架构并通过 EMA 更新的教师网络，iBOT 能够动态生成高质量的语义视觉词元作为掩码图像建模的目标。结合对 `[CLS]` 词元的自蒸馏，该方法成功地从未标注图像中学习到了强大的、具有良好泛化能力的视觉表示。实验结果表明，iBOT 在多项下游视觉任务上均取得了显著优于先前自监督方法的性能，证明了其设计思想的有效性。

从研究角度看，iBOT 的成功进一步凸显了以下几点：
1.  **高质量自监督信号的重要性**: 相比于预测低级特征或依赖固定词汇表，动态生成与当前模型能力相适应的、语义丰富的目标，对于提升自监督学习的上限至关重要。
2.  **知识蒸馏在自监督学习中的潜力**: 教师-学生架构，特别是通过 EMA 实现的自蒸馏，为稳定训练过程、避免表示坍塌和提炼知识提供了一个强大范式。
3.  **Transformer 架构在视觉领域的统一性**: iBOT 的成功再次证明了 Transformer 架构作为一种通用模型的潜力，能够有效地统一处理不同模态的数据，并通过类似 BERT 的预训练范式学习到强大的表示。

未来的研究方向可能包括：探索更高效的在线分词策略、将 iBOT 的思想扩展到多模态学习（如视觉-语言联合预训练）、以及研究如何进一步缩小自监督学习与有监督学习在更复杂任务上的性能差距。此外，理解 iBOT 学到的"视觉词元"的具体语义含义，以及它们与人类视觉感知的关联，也是一个值得探索的有趣方向。
