# 图像Tokenization发展路线：从基础理论到前沿应用

### 摘要

本报告深入探讨了图像Tokenization技术从基础概念到最先进应用的发展历程。图像Tokenization作为将原始像素数据转化为模型可理解离散单元的关键步骤，是Vision Transformer (ViT) 在计算机视觉领域取得突破的基石。报告重点介绍了ViT、BEiT、DINO、MAE和iBOT等里程碑式工作，详细阐述了它们在技术动机、创新设计、与现有工作的关联及改进、学术影响力等方面的贡献。这些模型通过引入Patch Embedding、离散视觉Token、自监督学习范式（如掩码图像建模和知识蒸馏）以及非对称编码器-解码器架构，显著克服了传统计算机视觉模型对大规模标注数据的依赖，并提升了计算效率和语义理解能力。报告进一步分析了当前图像Tokenization面临的样本质量、计算效率和更广泛适用性等挑战，并探讨了Subobject Tokenization、Set Tokenizer、SoftVQ-VAE等前沿方法如何通过更智能、更灵活的Tokenization策略来应对这些挑战。最终，报告总结了图像Tokenization在图像生成、多模态学习、视频理解和医学影像等领域的变革性应用，并展望了未来研究方向，强调了其在推动人工智能发展中的核心作用。

### 引言

图像Tokenization是计算机视觉领域的一个基础且关键的预处理步骤，其核心职责是将原始图像的像素信息转化为模型可理解的、离散的“Token”序列。这些Token在Transformer架构中扮演着类似自然语言处理（NLP）中词汇Token的角色，使得Transformer强大的自注意力机制能够捕捉图像内部的复杂空间关系和长距离依赖。这项技术已成为现代计算机视觉和人工智能的基础，显著提升了视觉数据处理的效率、可扩展性，并促进了跨模态应用的发展。

Transformer架构自2017年《Attention Is All You Need》论文发表以来，已成为自然语言处理（NLP）领域的标准模型。其通过自注意力机制捕捉序列中Token之间的长距离依赖关系，克服了传统循环神经网络（RNNs）如LSTM的顺序处理限制和梯度消失问题。NLP中成功的自监督预训练方法，如BERT的掩码语言建模（MLM）和GPT的自回归语言建模，展示了通过预测缺失内容来学习通用语言表示的巨大潜力。这种在NLP领域的巨大成功，直接促使研究者探索将Transformer范式引入到视觉领域，以克服传统卷积神经网络（CNNs）在捕捉全局上下文和长距离依赖方面的局限性。

将NLP的成功范式迁移到计算机视觉领域，不仅仅是架构上的复制，更是对核心概念（如“Token”）的深层再定义和适应。Transformer在NLP中通过大规模无监督预训练展现出强大的泛化能力，而早期的Vision Transformer（ViT）虽然强大，但对大规模标注数据表现出“数据饥渴”的特性。NLP中通过自监督学习从海量无标签数据中学习通用表示的经验，为视觉领域提供了解决“数据饥渴”的有效途径。因此，图像Tokenization的早期形式（如ViT的固定Patch）虽然实现了Transformer在图像上的应用，但其固有的语义不足和冗余性问题，直接催生了后续对“语义丰富”和“自适应”Tokenization方法的探索。这表明，图像Tokenization的发展路径，很大程度上是由弥合NLP和CV数据固有差异所驱动的，同时也是为了将NLP中“大规模无监督预训练+下游任务微调”的成功范式整体迁移到视觉领域，从而克服ViT对海量标注数据的依赖，并释放其在视觉任务中的潜力。

本报告旨在提供一个结构化的视角，阐明图像Tokenization模型如何克服了样本质量、计算效率和更广泛适用性方面的挑战，从而将其定位为人工智能领域的一项变革性技术。我们将追溯其发展路线，重点介绍关键的里程碑式论文，详细阐述它们的突破性创新、学术影响力及直接的arXiv链接。

### 基础理论与概念：图像Tokenization的演进

#### 图像Patch Tokenization的起源：Vision Transformer (ViT)

Vision Transformer (ViT) 于2020年提出，开创性地将图像视为一系列固定大小的二维Patch，并将其展平为一维序列，作为标准Transformer编码器的输入。这种方法摒弃了传统CNNs中固有的归纳偏置（如局部性、平移不变性），使得Transformer能够直接从全局自注意力机制中学习图像特征。每个展平的Patch通过可训练的线性投影映射到固定大小的潜在向量，形成“Patch Embedding”。为了保留Patch在原始图像中的位置信息，引入了可学习的1D位置编码，并将其添加到Patch Embedding中。ViT还借鉴BERT，在Patch序列前添加一个可学习的分类Token `[CLS]`，其在Transformer编码器输出时的状态被用作图像的整体表示进行分类。

ViT将图像Patch视为NLP中的“词”（tokens），并引入了分类Token，这与BERT中的 Token 功能相似。然而，尽管概念上相似，但图像Patch与文本Token存在本质区别。文本Token（如通过WordPiece或SentencePiece生成的子词）通常携带明确的语义信息，而图像的固定大小Patch往往包含语义不相关（如背景、天空）或需要更广阔上下文才能理解的像素。ViT的自注意力层是全局的，从最低层就能整合整个图像的信息，但其有意减少了图像特有的归纳偏置，这意味着大多数空间关系必须由模型从头学习。

ViT的创新在于减少了CNN固有的归纳偏置，直接应用Transformer。然而，在ImageNet等中等规模数据集上训练时，ViT的准确率低于ResNet。但当在JFT-300M等大规模数据集上预训练时，ViT能够匹配甚至超越SOTA的CNN。这揭示了一个核心权衡：CNN通过其卷积核和池化层内置了强大的局部性和平移不变性归纳偏置，使其在数据量有限时表现良好。而ViT缺乏这些强归纳偏置，需要通过“大规模数据预训练来弥补”，从而学习到更通用的、超越局部感知的特征。因此，ViT的Patch Tokenization不仅是技术创新，更是对模型设计哲学的一次重大转变，即用数据规模和通用架构来替代领域特定的归纳偏置。这种对大规模数据预训练的依赖，反向推动了自监督学习在视觉领域的爆发，使其成为ViT发展路线中不可或缺的一环。

#### 离散视觉Token的探索：Vector Quantized Variational AutoEncoder (VQ-VAE)

鉴于ViT中固定Patch的语义局限性，研究者开始探索如何为图像生成更具语义意义的离散Token。VQ-VAE（Vector Quantized-Variational AutoEncoder）于2017年被提出，其核心动机在于无监督地学习有用且离散的数据表示。它寻求在保留潜在空间重要特征的同时优化最大似然，并克服了传统VAE在与强大自回归解码器结合时常出现的“后验坍塌”（posterior collapse）问题，即潜在变量被忽略的问题。

VQ-VAE通过引入向量量化（Vector Quantization, VQ）机制，将连续的编码器输出映射到预定义的离散码本（codebook）中的最近向量。这个码本由有限数量的离散视觉Token组成，每个Token代表一种视觉概念或模式。通过这种方式，VQ-VAE能够有效避免传统VAE中常见的“后验坍塌”问题，确保离散潜在变量被模型有效利用，并能生成高质量的图像、视频和语音。其损失函数包含三部分：重建损失（优化解码器和编码器以重建输入）、VQ目标（通过L2误差学习嵌入空间）和承诺损失（确保编码器输出与嵌入匹配，防止其任意增长）。为了处理离散量化操作的不可导性，VQ-VAE采用了直通估计器（straight-through estimator）来近似梯度，使梯度信息能够有效回传到编码器。训练完成后，一个强大的自回归先验（如PixelCNN）会拟合离散潜在变量的分布，从而实现高质量的数据生成。

ViT的Patch是像素级的，缺乏语义。VQ-VAE的目标是学习“有用的离散表示”。VQ-VAE通过将连续图像特征量化为离散码本中的Token，实际上为图像提供了一种“视觉词汇表”。这个“词汇表”中的每个Token都代表了图像中某种抽象的视觉概念或模式，从而将低级像素信息提升到更高级的语义抽象层面。这种离散化不仅解决了Transformer处理连续图像数据的挑战，更重要的是，它为后续的掩码图像建模（MIM）等自监督任务提供了“语义目标”，使得模型能够学习到更具意义的视觉表示，而不是仅仅重建像素细节。VQ-VAE是实现图像“语义Tokenization”的关键一步，标志着图像Tokenization从纯粹的结构模仿向语义探索的演进。

### 里程碑式论文及其突破性创新

**表1: 关键里程碑论文概览**

| **论文名称**                                                 | **发表年份** | **核心创新**                                                 | **直观动机**                                                 | **与哪项已有工作最为相关，在哪些关键方面进行了改进或创新**   | **引用量 (Google Scholar)** | **arXiv链接**                                        |
| ------------------------------------------------------------ | ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | --------------------------- | ---------------------------------------------------- |
| Vision Transformer (ViT)                                     | 2020         | 将图像分割为固定大小的Patch并进行线性嵌入，引入可学习的分类Token和位置编码，直接将标准Transformer应用于图像。 | 挑战CNN在视觉领域的统治地位，探索纯Transformer在图像分类上的潜力。 | 借鉴NLP Transformer (Vaswani et al.,017; BERT, GPT-3)，但显著减少了视觉任务中的归纳偏置。 | 62844                       | [arXiv:2010.11929](https://arxiv.org/abs/2010.11929) |
| Neural Discrete Representation Learning (VQ-VAE)             | 2017         | 引入向量量化实现离散潜在变量，通过新颖的损失函数和直通估计器解决“后验崩溃”问题，并学习自回归先验。 | 无监督学习有用的离散表示，克服传统VAE的局限性。              | 改进了传统VAE框架，特别是其离散潜在变量的引入和对“后验崩溃”的解决。 | 6026                        | [arXiv:1711.00937](https://arxiv.org/abs/1711.00937) |
| BEiT: BERT Pre-Training of Image Transformers                | 2021         | 提出掩码图像建模（MIM），通过dVAE学习离散视觉Token，将MIM目标设定为预测这些语义Token，而非原始像素。 | 解决ViT对大规模标注数据的依赖，将BERT的MLM成功范式迁移到视觉领域，同时克服图像无“词汇表”及像素级预测低效的问题。 | 借鉴BERT的MLM，但创新性地引入了离散视觉Token作为预测目标，并使用dVAE进行Tokenization。 | 3437                        | [arXiv:2106.08254](https://arxiv.org/abs/2106.08254) |
| Emerging Properties in Self-Supervised Vision Transformers (DINO) | 2021         | 基于知识蒸馏的自监督方法，通过动量编码器、多裁剪训练、中心化和锐化操作防止模型崩溃。 | 探索自监督学习在ViT中产生的独特特性，无需标签学习高质量视觉表示，并发现其涌现的语义分割能力。 | 借鉴BYOL等自监督框架，但将其应用于ViT并引入了中心化和锐化机制以防止模型崩溃，从而在ViT中实现了显著的自监督学习效果。 | 7200                        | [arXiv:2104.14294](https://arxiv.org/abs/2104.14294) |
| Masked Autoencoders Are Scalable Vision Learners (MAE)       | 2021         | 提出非对称编码器-解码器架构，编码器仅处理可见Patch，解码器轻量化且在编码器输出后引入掩码Token，并采用高达75%的高掩码比例。 | 进一步提升自监督学习在视觉领域的扩展性，解决图像数据冗余性高的问题，实现更高效、可扩展的视觉学习。 | 借鉴BERT的MLM，但创新性地采用非对称架构和高掩码比例，使其在图像领域更高效且能学习到高层语义。与BEiT在预测目标上有所不同（像素 vs. 视觉Token）。 | 9933                        | [arXiv:2111.06377](https://arxiv.com/abs/2111.06377) |
| iBOT: Image BERT Pre-Training with Online Tokenizer          | 2021         | 将MIM框架化为带有在线Tokenizer（教师网络）的知识蒸馏问题，通过EMA机制联合学习Tokenizer，并结合CLS Token的自蒸馏来获取视觉语义，使用软标签监督处理图像Patch的语义模糊性。 | 克服固定预训练Tokenizers的局限性，实现视觉Tokenizer与主模型的联合优化，学习更丰富的局部语义模式。 | 借鉴BEiT的MIM和DINO的自蒸馏思想，但创新性地实现了Tokenizer的“在线”学习和联合优化，避免了多阶段训练。 | 1044                        | [arXiv:2111.07832](https://arxiv.org/abs/2111.07832) |
| Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | 2021         | 提出分层Transformer架构，通过“移位窗口”机制限制自注意力计算在局部窗口内，同时实现跨窗口连接。 | 解决传统ViT在处理高分辨率图像时计算复杂度高（二次方增长）和多尺度视觉实体建模的挑战。 | 改进了ViT的全局自注意力机制，通过局部窗口和移位窗口实现了线性计算复杂度，并具备多尺度建模能力，使其成为通用视觉骨干。 | 30895                       | [arXiv:2106.03797](https://arxiv.org/abs/2106.03797) |



#### Vision Transformer (ViT): 图像Transformer的奠基石

ViT的核心动机是挑战卷积神经网络（CNNs）在计算机视觉领域的长期主导地位，并探索Transformer架构能否直接应用于图像处理，从而实现图像分类任务中的优异性能。其技术设计直观地体现了这一动机：将图像分解为一系列固定大小的非重叠Patch，并将这些Patch展平并进行线性嵌入，形成一个序列，就像NLP中处理词语序列一样。这种设计显著减少了图像领域特有的归纳偏置，迫使模型从数据中学习更通用的视觉模式。

ViT与NLP中的Transformer模型（如BERT和GPT-3）最为相关，直接借鉴了其编码器架构和自注意力机制。关键改进与创新包括：首次将图像直接分割成固定大小的Patch作为Transformer的输入“Token”，而非使用卷积层提取特征；引入可学习的1D位置编码来保留Patch在原始图像中的空间信息；借鉴BERT的`[CLS]` Token，在Patch序列前添加一个可学习的嵌入，其最终输出用于图像分类。相较于CNN，ViT的自注意力机制是全局的，从底层即可整合全图信息，这使得模型需要从大规模数据中学习空间关系，而非依赖预设的局部性偏置3。ViT的学术影响力巨大，引用量高达909317，其arXiv链接为[arXiv:2010.11929](https://arxiv.org/abs/2010.11929)。

ViT的成功依赖于大规模数据预训练。然而，获取大规模标注数据成本高昂2。这引出了一个必然的趋势：为了充分发挥ViT的潜力并克服其“数据饥渴”问题，研究社区必须转向更高效、更可扩展的无监督或自监督学习方法。ViT的成功，反向推动了自监督学习在视觉领域的爆发，使其成为ViT发展路线中不可或缺的一环。

#### BEiT: 掩码图像建模的先驱

BEiT的动机在于解决ViT对大规模标注数据的依赖问题，并借鉴BERT在NLP中通过掩码语言建模（MLM）进行自监督预训练的成功经验1。然而，直接将MLM应用于图像面临挑战：图像没有预定义的“词汇表”，且像素级预测（回归问题）效率低下，无法有效学习高层语义1。为解决此问题，BEiT引入了“视觉Token”的概念，并通过离散变分自编码器（dVAE）预先学习一个视觉码本，将图像像素转换为离散的视觉Token1。其技术设计体现为：随机掩盖部分图像Patch，然后训练Transformer预测这些被掩盖Patch对应的原始视觉Token9。

BEiT与BERT的掩码语言建模最为相关，是其在视觉领域的直接扩展1。关键改进与创新包括：创新性地使用dVAE学习到的离散视觉Token作为掩码预测的目标，而非原始像素值1。这使得预训练任务从低级像素重建转向高级语义理解，显著提升了模型学习高层抽象特征的能力1。BEiT采用两阶段训练：首先训练dVAE获取视觉Tokenizer，然后固定Tokenizer进行MIM预训练0。此外，它还采用块级掩码策略，而非随机单个Patch掩码，这在NLP的BERT类模型中也有应用1。BEiT的学术影响力显著，引用量高达583319，其arXiv链接为[arXiv:2106.08254](https://arxiv.org/abs/2106.08254)1。

BEiT明确提出像素级预测效率低下，转而预测语义视觉Token1。然而，MAE后来成功地通过像素级重建实现了SOTA，甚至有研究指出MAE“部分矛盾”了BEiT的结论0。这表明在图像Tokenization中，关于“预测什么”以及“如何最有效学习语义”存在不同的路径和权衡。BEiT通过引入离散语义Token，试图将图像预训练任务提升到更高语义层次，从而避免了对低级细节的过度关注。然而，MAE通过巧妙的非对称架构和高掩码比例，证明了即使是像素级重建，如果设计得当，也能迫使模型学习高层语义。这一对比揭示了视觉Tokenization领域的一个深层问题：语义化并非只有一种实现方式，效率和语义深度之间存在复杂的相互作用。

#### DINO: 自监督学习中的涌现特性

DINO的动机是探索自监督学习在Vision Transformer中是否能产生独特的、超越传统CNN的特性，从而无需大量标签数据即可学习高质量的视觉表示2。其技术设计体现为一种基于知识蒸馏的自监督方法，其中一个“学生网络”通过预测“教师网络”的输出来学习。教师网络是学生网络的动量更新版本，并且不接受直接梯度回传2。为防止模型学习到平凡解（即所有输出都相同，导致“模式崩溃”），DINO引入了中心化（centering）和锐化（sharpening）操作来规范教师网络的输出分布2。

DINO与BYOL、MoCo、SwAV等现有的自监督学习框架共享整体结构，并与知识蒸馏方法有相似之处2。关键改进与创新包括：将自监督学习解释为一种“无标签的自蒸馏”形式，学生网络学习匹配动量教师网络的输出2；引入中心化和锐化操作，有效防止了模型崩溃，这在不使用Batch Normalization的ViT中尤为重要2；发现DINO与ViT结合时表现出显著的协同效应，使得模型能够学习到显式的语义分割信息，即使没有进行监督训练2；训练后的ViT特征作为k-NN分类器表现出色，无需微调或线性分类器即可达到高准确率2。DINO的学术影响力显著，引用量高达71150，其arXiv链接为[arXiv:2104.14294](https://arxiv.org/abs/2104.14294)1。

DINO的自监督ViT特征包含“显式语义分割信息”1，并且DINO训练的ViT能“学习到语义分割对象并创建边界”，且信息在自注意力模块中可访问2。这不仅仅是提高了性能，更重要的是揭示了自监督学习在ViT中能够诱导出超越传统分类任务的、更深层次的视觉理解能力，例如对物体轮廓和语义区域的自动感知。这种“涌现能力”为视觉Transformer的可解释性提供了新的视角，表明模型在无监督的情况下也能学习到人类直观理解的视觉概念，这对于推动通用人工智能的发展具有深远意义。

#### MAE: 可扩展的视觉学习器

MAE的动机是解决深度学习（特别是计算机视觉）对大规模标注数据的巨大需求，以及图像数据中固有的高空间冗余性问题。它旨在将NLP中掩码自编码器（如BERT）的成功和可扩展性带到视觉领域。其技术设计体现为：通过掩盖图像中高达75%的随机Patch来创建一个非平凡的自监督任务，迫使模型学习全局理解而非仅仅局部推断。

MAE与BERT的掩码语言建模（MLM）以及其他掩码图像建模（MIM）方法（如BEiT）最为相关。关键改进与创新包括：提出非对称编码器-解码器架构，编码器仅处理可见的图像Patch，不引入掩码Token，显著提高了训练效率（3倍或更多加速）并减少了计算量。掩码Token仅在轻量级解码器阶段引入，用于重建原始像素。MAE采用高达75%的随机掩码比例，这远高于NLP中的BERT（15%）和其他视觉方法（20-50%），旨在消除图像的冗余性，迫使模型进行高级语义推理。尽管BEiT认为像素级预测效率低下，MAE通过其非对称架构和高掩码比例，证明了直接重建原始像素也能学习到高质量的语义表示。MAE的学术影响力巨大，引用量高达98712，其arXiv链接为[arXiv:2111.06377](https://arxiv.org/abs/2111.06377)。

MAE的核心创新是高掩码比例和非对称架构。高掩码比例增加了重建任务的难度，迫使模型学习全局语义。非对称架构则通过只让编码器处理可见Patch来提高效率。这揭示了在自监督学习中，通过精心设计任务难度（高掩码）和模型架构（非对称），可以实现效率和学习效果的平衡。高难度任务能促使模型学习更深层次的特征，而高效架构则确保了这种学习在大规模数据和模型上是可行的。这种平衡是推动Transformer在视觉领域可扩展性的关键。

#### iBOT: 在线Tokenization的探索

iBOT的动机源于NLP中掩码语言建模（MLM）的成功，其依赖于语义有意义的Token。然而，在图像领域，缺乏现成的、语义有意义的视觉Tokenizer，且现有方法（如BEiT中的预训练dVAE）需要多阶段训练且Tokenizer是固定的4。iBOT旨在克服这些限制，开发一个单阶段训练流程，使视觉Tokenizer与主模型能够联合优化，从而在线获取高层视觉语义4。其技术设计体现为：将MIM任务框架化为知识蒸馏问题，其中教师网络充当“在线Tokenizer”，它与学生网络一起通过动量更新机制共同学习4。

iBOT与BEiT的MIM方法和DINO的自蒸馏思想（特别是使用`Token进行自蒸馏）最为相关4。关键改进与创新包括：教师网络作为在线Tokenizer，其参数通过学生网络的指数移动平均（EMA）进行更新，实现了Tokenizer与MIM目标的联合学习，避免了多阶段训练4；结合对跨视图图像的` Token进行自蒸馏，以获取全局视觉语义，并发现共享``和Patch Token投影头有助于性能提升4；使用Softmax后的Token分布（软标签）作为监督信号，而非硬标签，以处理图像Patch的语义模糊性，这被认为是提升预训练性能的关键4。iBOT的学术影响力为1049次引用3，其arXiv链接为[arXiv:2111.07832](https://arxiv.org/abs/2111.07832)4。

BEiT使用预训练的dVAE作为固定Tokenizer1。iBOT的动机是克服这种固定Tokenizer的局限性4。iBOT通过“在线Tokenizer”和联合学习机制，使得Tokenizer本身能够随着训练数据的变化而动态调整其语义理解，从而更好地适应特定数据集的视觉特征。这种从固定到动态Tokenization的转变，代表了图像Tokenization在语义适应性方面的重要进步，使得模型能够学习到更符合当前任务和数据分布的、更丰富的局部语义模式。

#### Swin Transformer: 分层视觉Transformer

Swin Transformer的动机是为了将Transformer架构成功应用于计算机视觉，同时解决传统ViT在处理高分辨率图像时计算复杂度高（二次方增长）以及视觉实体尺度变化大等挑战5。其技术设计体现为：引入一种“分层”结构，通过“移位窗口”（shifted windows）机制限制自注意力计算在非重叠的局部窗口内，从而将计算复杂度从图像尺寸的二次方降低到线性5。同时，移位窗口机制确保了跨窗口的信息连接，从而捕获全局上下文5。

Swin Transformer是对ViT的重大改进，旨在使其更适应计算机视觉任务的特性5。关键改进与创新包括：借鉴CNN的金字塔结构，Swin Transformer构建了多阶段的分层特征表示，使其能够处理不同尺度的视觉信息，适用于密集预测任务5；核心创新是移位窗口自注意力，通过在连续层之间交替使用常规和移位窗口，实现了局部注意力计算的效率和全局信息交互5；解决了ViT全局自注意力导致的二次方复杂度问题，使其能够处理更高分辨率的图像，并作为通用视觉骨干5。Swin Transformer的学术影响力为14871次引用5，其arXiv链接为[arXiv:2106.03797](https://arxiv.org/abs/2106.03797)6。

ViT最初通过减少归纳偏置来追求通用性3。Swin Transformer则通过引入“分层”和“移位窗口”机制，重新引入了类似CNN的局部性和层次性归纳偏置5。这种再引入并非倒退，而是为了解决ViT在实际应用中遇到的效率和多尺度建模问题。它表明，在某些情况下，适度地将领域特定的结构性归纳偏置融入通用Transformer架构，可以显著提升性能和效率，使其更适合作为“通用视觉骨干”，尤其是在密集预测任务中。这是一个从纯粹通用到“通用且高效”的演变。

### 挑战、改进与最新进展

#### 克服样本质量与计算效率挑战

图像Tokenization领域持续面临如何高效且高质量地将图像信息转化为Token的挑战。ViT的“数据饥渴”问题促使了自监督学习的兴起，但即使有了SSL，ViT及其变体仍然面临计算效率和模型复杂度的挑战。研究工作正从最初的“需要大量数据才能工作”转变为“如何在有限资源下更高效地工作，并学习到更精炼的语义”。

在高效Tokenization策略方面，研究者提出了多种创新。ImagePiece针对ViT中图像Patch语义不足的问题，提出了一种新颖的重Tokenization策略，借鉴NLP的MaxMatch思想，将语义不足但局部连贯的Token进行分组，显著减少相关Token数量，提高推理速度（DeiT-S提速54%），同时提升ImageNet分类精度4。Subobject-level Adaptive Token Segmentation引入了子对象级别的自适应Token分割，例如使用超像素或SAM等方法，生成更具有单义语义的Token，实现更快的收敛和更好的泛化能力，同时使用更少的视觉Token。ViT-1.58b针对ViT高计算和内存需求在资源受限环境部署的挑战，引入了1.58比特量化，大幅降低内存和计算开销，同时保持竞争力性能。SoftVQ-VAE解决了高压缩比下图像Tokenization的效率和表示能力问题，通过软分类后验聚合多个码字到每个潜在Token中，大幅增加了潜在空间的表示能力，显著提升了生成模型的推理吞吐量（最高达55倍）和训练效率。SparseSwin在Swin Transformer的基础上，通过引入Sparse Transformer Block来减少Transformer模型中的参数数量和Token使用量，从而提高模型的效率并降低复杂性。

在改进的自监督学习范式方面，DiTo（Diffusion Tokenizer）提出了一种简单的扩散Tokenizer，仅使用单一的扩散L2损失即可学习紧凑的视觉表示，从而简化了Tokenizers的训练流程，提供了一种更简单、可扩展的自监督替代方案。MiCL（Masked Image Contrastive Learning）通过随机掩码图像Patch来创建具有细粒度语义差异的视图，并进行对比学习，从而减少了概念冗余并提高了效率，无需额外的辅助模块7。这些方法通过量化、智能Token分组/分割、高压缩比和简化训练目标，系统性地解决了效率瓶颈，标志着图像Tokenization发展进入了一个“高效精炼”的阶段，目标是在保持甚至提升性能的同时，显著降低计算成本和数据需求，从而使这些强大的模型更具实用性。

#### 更广泛的领域应用

图像Tokenization最初主要服务于图像分类任务，但其应用已迅速扩展到更广泛的领域，展现出强大的泛化能力。

在**图像生成**方面，DiffiT将Vision Transformer应用于扩散模型，实现了高保真图像生成，通过引入时间依赖多头自注意力（TMSA）机制，实现了对去噪过程的精细控制，在ImageNet256数据集上取得了新的SOTA FID分数，同时参数效率更高。Set Tokenizer提出了一种根本不同的图像表示方法，将图像表示为无序的Token集合，而非固定空间位置的序列，能够根据语义复杂性动态分配编码容量，提高了对局部扰动的鲁棒性，并实现了排列不变性和全局上下文感知，从而生成更连贯的图像。

在**多模态理解与生成**方面，图像Tokenization是连接视觉与其他模态的关键接口。GaussianToken针对现有离散码本空间有限、限制表示能力的问题，提出了一种基于2D高斯Splattings的图像Tokenizer，扩展了原始离散空间的表示能力，对于多模态理解和生成至关重要6。QLIP结合了SOTA的重建质量和零样本图像理解能力，可作为LLaVA的视觉编码器或LlamaGen的图像Tokenizer的直接替代品，突显了图像Tokenizer在多模态对齐中的关键作用1。MM-Interleaved解决了多模态LLMs在有限上下文长度内处理多图像的挑战，引入了多尺度和多图像特征同步器模块，高效地从多个图像中观察多尺度高分辨率特征，适用于交错的图像-文本场景2。

在**视频理解**方面，LVMAE（Long-Video Masked Autoencoder）将MAE预训练扩展到长视频序列（128帧），解决了现有视频MAE受限于短视频（16/32帧）的问题，显著提升了视频理解任务的性能3。UniViTAR针对传统ViT标准化输入分辨率、忽视自然视觉数据可变性的问题，提出了一种能够同时支持原生分辨率和固定分辨率处理的Transformer骨干，提高了对可变长度视觉序列的适应性4。这表明图像Tokenization正在向“时空Tokenization”发展，旨在捕捉和表示更复杂的时空动态信息。这种泛化能力是推动视觉AI从静态图像理解走向动态世界理解的关键一步。

在**医学影像**等专业领域，图像Tokenization也展现出巨大潜力。X-LRM在稀疏视图CT重建中应用图像Tokenization，使用Transformer-based编码器生成输出Token，然后将其上采样为3D隐式神经场，在速度和灵活性上均超越SOTA，并对肺部分割等下游任务具有实际价值5。此外，在医学影像领域，Tokenization不仅限于图像本身，还涉及对相关文本（如放射学报告）的Tokenization。研究发现，即使是特定领域的词汇表选择也会影响下游分类性能，凸显了在专业领域中Tokenizer设计的重要性6。这表明图像Tokenization的发展不仅仅是通用能力的提升，更是向特定领域（如医疗）的深度适应，并开始与该领域的其他模态（如文本报告）进行更紧密的融合，以提供更全面的智能解决方案。

### 结论与展望

图像Tokenization已从最初简单的Patch分割发展成为高度复杂且语义丰富的视觉表示技术，深刻改变了计算机视觉和更广泛的AI领域。它使得Transformer架构能够有效处理图像数据，克服了传统CNN的局限性，并在自监督学习的推动下，实现了模型的可扩展性和泛化能力的大幅提升。从ViT的开创性工作到BEiT、DINO、MAE、iBOT等在自监督预训练中的突破，再到Swin Transformer在效率和多尺度建模上的创新，图像Tokenization持续推动着视觉AI的边界。其应用已从基础的图像分类扩展到高保真图像生成、复杂的密集预测任务、多模态理解与生成、视频分析以及专业领域的医学影像等，展现了其作为核心接口技术的巨大潜力。

展望未来，图像Tokenization的研究将聚焦于以下几个关键方向：

- **更自适应和动态的Tokenization：** 进一步探索能够根据图像内容、语义复杂性或特定任务需求动态调整Token粒度、数量和结构的Tokenizer。例如，从固定网格Patch转向基于对象或语义区域的自适应分割。
- **极致的效率与压缩：** 持续优化Tokenization过程，实现更高的图像压缩比，同时保持甚至提升信息保真度和语义完整性，以应对大规模模型和资源受限环境的挑战。
- **多模态深度融合的接口：** 随着多模态AI的兴起，图像Tokenizer将扮演更重要的角色，作为连接视觉与其他模态（如文本、音频、3D数据）的桥梁，实现更深层次的跨模态理解和生成。这包括开发能够无缝处理交错多模态输入并保持细粒度信息流动的Tokenizer。
- **可解释性与可控性：** 深入研究Tokenization过程如何影响模型的内部表示和决策，提高视觉Token的可解释性，并探索更精细的控制机制，以指导模型学习特定类型的视觉概念，从而增强模型在复杂任务中的可靠性和透明度。

图像Tokenization的发展路线清晰地描绘了一个从模仿到创新、从通用到高效、从单模态到多模态的演进过程。其持续的进步将是推动人工智能迈向更智能、更通用、更具应用价值未来的关键驱动力。